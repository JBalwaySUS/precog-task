# Precog Recruitment Task 2025
Name: Anirudh Sankar \
Roll Number: 2023111024

## Using the notebook

### Clone the repository
```bash
git clone https://github.com/JBalwaySUS/precog-task.git
```

### Create a virtual environment (RECOMMENDED)
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### Explaining the folder structure
`cooccur_matrix`: Contains the co-occurrence matrix generated using the English News 2020 1M Corpus of the Leipzig Corpora Collection. 

`dictionaries`: Contains pickled dictionaries and csv files for the co-occurrence matrix, unique token identifiers and glove_embeddings. \
(All generated by notebook, no need for preloading) 

`eng_news_2020_1M`: Contains the English News 1M Corpus of the Leipzig Corpora Collection. (PRELOAD REQUIRED) \

`fastText`: Contains the fastText embeddings for the English and Hindi language in both "300-dimensions Common Crawl" and "Wikipedia" trained. (PRELOAD REQUIRED) \

`hin_news_2020_1M`: Contains the Hindi News 1M Corpus of the Leipzig Corpora Collection. (PRELOAD REQUIRED) \

`MEN`: Contains the MEN dataset for evaluation of embeddings. (PRELOAD REQUIRED) \

`MUSE_bilingual_dictionary`: Contains the MUSE bilingual dictionary for English-to-Hindi and Hindi-to-English. (PRELOAD REQUIRED) \

`SimLex-999`: Contains the SimLex-999 dataset for evaluation of embeddings. (PRELOAD REQUIRED) \

`tensorboard-visualizations`: Contains the checkpoints for tensorboard visualizations for the embeddings. (Generated during runtime) \

`word_embeddings`: Contains the embeddings extracted from the co-occurrence matrix and t-SNE reduced matrices for plotting. (Generated during runtime) \

`wordsim353`: Contains the wordsim353 dataset for evaluation of embeddings. (PRELOAD REQUIRED) \

#### Other preloaded files
`glove.840B.300d.txt`: Contains the GloVe embeddings for the English language. (PRELOAD REQUIRED) \
`GoogleNews-vectors-negative300.bin`: Contains the Word2Vec embeddings trained on Google News for the English language. (PRELOAD REQUIRED)

### Install the required packages
Run the first cell of the notebook to install the required packages.

### Done!

## Important Note:
Most cells of the notebook are designed to be run as a single program on its own. This may lead to kernel crashes due to memory contraints. Variable deletion and garbage collection have been implemented to prevent this as much as possible.

# Language Representations
## Part 1: Dense Representations
### 1. Creating Co-occurrence Matrix
My first approach was to create a co-occurrence matrix using the English News 2020 1M Corpus of the Leipzig Corpora Collection. The matrix was created using a window size of 5.

The corpus was tokenized using the `nltk` library. After all the unique tokens were identified, a dictionary was created to map the tokens to unique identifiers.

Intial idea was to use numpy.zeros to create a matrix of size (len(tokens), len(tokens)). However, when I saw I would need 693 GB of memory, a new approach was needed.

I then stored only the non-zero values of the matrix in a dictionary whose key was of the form (row, column) and value was the count of the co-occurrence. This gave me the insight that the matrix was very sparse.

To reduce dimentionality, I needed to perform SVD on the matrix. The scipy implementation of SVD is the fastest {https://stackoverflow.com/questions/32569188/scipy-svd-vs-numpy-svd}. So I needed to use scipy to store the sparse matrix. The two most popular representations are CSR and COO.

CSR (Compressed Sparse Row)
- Faster in row vector operations
- Cannot extract coordinate information of non-zero entries

COO (Coordinate)
- Slower in calculations but supported by PyTorch (CSR is still in beta phase)
- Coordinate information of non-zero entries are preserved

Since I wanted to extract the coordinate information of non-zero entries, I chose COO.

The token-to-identifier dictionary, the co-occurrence matrix dictionary was pickled and saved for future use.
The Scipy Sparse COO Array was also saved as a npz file.

### 2. Dimensionality Reduction
As stated earlier I used Scipy's `TruncatedSVD` function to calculated the SVD of the co-occurrence matrix. I generated embeddings of multiple dimensions 

The extracted embeddings were saved using NumPy's `save` function.


### 3. Evaluate the quality of the embeddings
The quality of the embeddings were evaluated using three datasets:

1) MEN dataset
2) SimLex-999
3) WordSim-353

Used Numpy to calculate cosine similarity between the embeddings and `scipy.stats.spearmanr` to calculate the Spearman's rank correlation coefficient.

The same approach was used for evaluating the pre-trained embeddings.

BERT and word2vec are exceptions to this
- Gensim provide a function to calculate the cosine similarity between word2vec embeddings. 
- For BERT, the notion of cosine similarity does not exist hence I used the `transformers` library provided by Hugging Face to calculate the confidence score of the transformer model using the `fill-mask` pipeline.

The results for these evaluations are included in the report.

#### Visualizing the embeddings
For 2D Plot I used `scikit-learn` to make `TSNE` Manifolds to reduce the dimensions of the embeddings to 2-dimensions and 3-dimensions. The embeddings were then plotted using `matplotlib` and `plotly` respectively.

An aditional visualization was done using `tensorboard` to visualize the embeddings in 3D while being interactive enabling the user to see nearest neighbours of words.

Additional steps taken for visualizations are:
- Plotly requires a pandas dataframe to plot the embeddings. So I converted the embeddings to a pandas dataframe.
- Tensorboard requires a PyTorch or TensorFlow tensor to plot the embeddings. So I converted the embeddings to a PyTorch tensor using `torch.from_numpy`. And then used `torch.utils.tensorboard.SummaryWriter` to write the embeddings to a tensorboard log file to be visualized.

## Part 2: Cross-lingual Alignment

For this part I used the MUSE bilingual dictionary for English-to-Hindi and Hindi-to-English. The embeddings were aligned using the Procrustes method.

The iterative Procrustes method was implemented by me using Facebook Reasearch's MUSE as reference (https://github.com/facebookresearch/MUSE)

The implementation uses FAISS to speed up the nearest neighbour search. The embeddings were aligned using the Procrustes method and the results were evaluated using the MUSE bilingual dictionary.

## Bonus Task: Harmful Association

To evaluate the harmful associations, I used he The Word Embedding Fairness Evaluation (WEFE) Python package.

It provides datasets for Word Embedding Association Tests (WEAT).

I use the WEAT test to evaluate the harmful associations in the embeddings.
Using the cosine similarity for word2vec embeddings and the fill-mask confidence score for BERT embeddings.